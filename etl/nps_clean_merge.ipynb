{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1-SjdmrBvrQyCBCrs4h0kO_pyjWIQQArY",
      "authorship_tag": "ABX9TyOCFmAwO/fiv281wcHLYRhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkumar02/Tourist-Flow-And-Seasonality-Analyzer-With-Event-Impact/blob/main/etl/nps_clean_merge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWNMNmGDkRHn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Capstone project/datasets for monthly recreation visit to national parks/Query builder files\""
      ],
      "metadata": {
        "id": "BQlfRf6Hok4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excel_files = [f for f in os.listdir(folder_path) if f.endswith(('.xlsx','.xls'))]"
      ],
      "metadata": {
        "id": "UU9Q-XpVo6OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excel_files.sort()"
      ],
      "metadata": {
        "id": "0HS-J3Auo7QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_excel_files(folder_path,excel_files):\n",
        "  print(f\"\\nTotal DataFrames created: {len(excel_files)}\")\n",
        "  print('\\n\\n\\n')\n",
        "  for i, file in enumerate(excel_files, start=1):\n",
        "    path = os.path.join(folder_path, file)\n",
        "    globals()[f'df{i}'] = pd.read_excel(path,skiprows=2)\n",
        "    print('\\n')\n",
        "    print('File loading')\n",
        "    print(f\"Loaded {file} â†’ df{i}  (rows: {globals()[f'df{i}'].shape[0]}, cols: {globals()[f'df{i}'].shape[1]})\")"
      ],
      "metadata": {
        "id": "9AWDh60jo--U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reading_excel_files(folder_path,excel_files)"
      ],
      "metadata": {
        "id": "Ysr2BWti5W5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nTotal DataFrames created: {len(excel_files)}\")"
      ],
      "metadata": {
        "id": "cfhBaeQdpEOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def understanding_data(df):\n",
        "  print('First 5 rows of the dataset')\n",
        "  print('\\n')\n",
        "  print(df.head())\n",
        "  print('\\n\\n\\n')\n",
        "  print('Shape of the dataset')\n",
        "  print('\\n')\n",
        "  print(df.shape)\n",
        "  print('Information about the dataset')\n",
        "  print('\\n')\n",
        "  print(df.info())\n",
        "  print('\\n\\n\\n')\n",
        "  print('Description of the dataset')\n",
        "  print('\\n')\n",
        "  print(df.describe())\n",
        "  print('\\n\\n\\n')"
      ],
      "metadata": {
        "id": "VEEKnEWkHGRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleaning(df):\n",
        "  print('Checking for number of null values in each columns')\n",
        "  print('\\n')\n",
        "  print(f'Total no of null values in each  column: {df.isnull().sum()}')\n",
        "  print('\\n')\n",
        "  print('Checking for null values in each row')\n",
        "  print('\\n')\n",
        "  null_rows=df[df.isnull().any(axis=1)]\n",
        "  print(f\" Total number of rows with null values: {len(null_rows)}\")\n",
        "  print(\"\\n\")\n",
        "  print(\"Rows with null values\")\n",
        "  print(\"\\n\")\n",
        "  print(null_rows.to_string(index=False))\n",
        "  print('\\n\\n\\n')\n",
        "  print('Checking for duplicate rows')\n",
        "  print('\\n')\n",
        "  print(f'number of duplicate values: {df.duplicated().sum()}')\n",
        "  print('\\n')\n",
        "  print('Duplicated rows in the dataset')\n",
        "  print('\\n')\n",
        "  print(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "FNMkvK578Ee-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outliner_detection(df):\n",
        "  numeric_df = df.select_dtypes(include=np.number)\n",
        "  Q1=numeric_df.quantile(0.25)\n",
        "  Q3=numeric_df.quantile(0.75)\n",
        "  IQR=Q3-Q1\n",
        "  outliers=numeric_df[((numeric_df<(Q1-1.5*IQR))|(numeric_df>(Q3+1.5*IQR))).any(axis=1)]\n",
        "  print('Number of outliers in each column')\n",
        "  print('\\n')\n",
        "  print(outliers.count())\n",
        "  print('\\n')\n",
        "  print('Outliers in the dataset')\n",
        "  print('\\n')\n",
        "  print(outliers)"
      ],
      "metadata": {
        "id": "d6ctjFUDHxOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replacing_nan_values(df,column_name,value):\n",
        "  df[column_name].fillna(value,inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "NVmyplGUlWyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "understanding_data(df10)"
      ],
      "metadata": {
        "id": "JTQooG_O-ICU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaning(df29)"
      ],
      "metadata": {
        "id": "HUSzIlAbTc7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df6,'State','AK')"
      ],
      "metadata": {
        "id": "n4myqAdLk1NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df9,'State','CO')"
      ],
      "metadata": {
        "id": "e07TbbfsITBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df24,'State','DC')"
      ],
      "metadata": {
        "id": "I9tc8zIk9ZzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df39,'State','MA')"
      ],
      "metadata": {
        "id": "eg2hS4i49Eb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df47,'State','KY')"
      ],
      "metadata": {
        "id": "8Q25r23L1VWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df59,'State','DC')"
      ],
      "metadata": {
        "id": "g4DNbUvB1n39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df65,'State','CA')"
      ],
      "metadata": {
        "id": "kH3lpvvK16yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df70,'State','OH')"
      ],
      "metadata": {
        "id": "kfj4qLpW2Fm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df99,'State','DC')"
      ],
      "metadata": {
        "id": "604q9-r42Zwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df106,'State','MS')"
      ],
      "metadata": {
        "id": "N1N7K5IV2out"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df112,'State','DE')"
      ],
      "metadata": {
        "id": "nUJqDvHi2-3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_nan_values(df176,'State','MD')"
      ],
      "metadata": {
        "id": "ddlyeJLbAAND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_dataframe_construction(*dfs):\n",
        "  df=pd.concat(dfs,ignore_index=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "hdo-rS_PZEPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_numbered_dfs(globals_dict):\n",
        "    \"\"\"Return a sorted list of DataFrames from globals whose names match df<number>.\"\"\"\n",
        "    pattern = re.compile(r'^df(\\d+)$')   # strict: df followed by digits only\n",
        "    matched = []\n",
        "    for name, val in globals_dict.items():\n",
        "        if isinstance(val, pd.DataFrame):\n",
        "            m = pattern.match(name)\n",
        "            if m:\n",
        "                num = int(m.group(1))\n",
        "                matched.append((num, name, val))\n",
        "    if not matched:\n",
        "        return []\n",
        "    # sort by numeric suffix and deduplicate by numeric index if duplicates exist\n",
        "    matched.sort(key=lambda t: t[0])\n",
        "    seen = set()\n",
        "    ordered_dfs = []\n",
        "    for num, name, df in matched:\n",
        "        if num in seen:\n",
        "            # prefer first occurrence (you can change to prefer last)\n",
        "            continue\n",
        "        seen.add(num)\n",
        "        ordered_dfs.append((num, name, df))\n",
        "    # return only the DataFrame objects in order\n",
        "    return [t[2] for t in ordered_dfs]"
      ],
      "metadata": {
        "id": "IORGT0FZreiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs_to_concat = collect_numbered_dfs(globals())"
      ],
      "metadata": {
        "id": "Q5zHYjrNrfrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_dataframe_construction(*dfs_to_concat)"
      ],
      "metadata": {
        "id": "h7Y1XNIcr2MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "understanding_data(final_df)"
      ],
      "metadata": {
        "id": "dWTsGNAacXIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaning(final_df)"
      ],
      "metadata": {
        "id": "4tHm_VnYHhpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv('/content/drive/MyDrive/final_dataset.csv')"
      ],
      "metadata": {
        "id": "27ca8ubGpfqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}