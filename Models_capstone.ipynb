{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "16GuZ_MTuEMqG6BNOkigz9wvdgH-0j_Kr",
      "authorship_tag": "ABX9TyNR4KJAAlKLaiqAmYiBvwWY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkumar02/Tourist-Flow-And-Seasonality-Analyzer-With-Event-Impact/blob/pranavkumar02-patch-2/Models_capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "WKCEWhQF9vDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "C5dBKNbz9y0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "DATA_PATH = \"/content/drive/MyDrive/final_dataset.csv\"\n",
        "OUTPUT_DIR = Path(\"./forecast_pipeline_output\")\n",
        "FORECASTS_DIR = OUTPUT_DIR / \"forecasts\"\n",
        "PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
        "RESULTS_CSV = OUTPUT_DIR / \"model_selection_results.csv\"\n",
        "\n",
        "HORIZON = 12  # months to forecast / test horizon\n",
        "MIN_HISTORY_MONTHS = 36  # minimum months of history to attempt seasonal models\n",
        "LSTM_MIN_MONTHS = 60  # require more history for LSTM\n",
        "LSTM_EPOCHS = 100\n",
        "LSTM_BATCH = 16\n",
        "LSTM_LOOKBACK = 12  # months window for LSTM\n",
        "USE_LOG_TRANSFORM = False  # set True to use log1p transform (remember inverse when evaluating)"
      ],
      "metadata": {
        "id": "y-9Iva8F92WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Which models to run (set to False to skip heavy models)\n",
        "RUN_ETS = True\n",
        "RUN_PROPHET = True\n",
        "RUN_SARIMA = True\n",
        "RUN_LSTM = False"
      ],
      "metadata": {
        "id": "6yQwqZCL-Us5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dirs\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FORECASTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "g2yKGoJt-axU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(DATA_PATH)"
      ],
      "metadata": {
        "id": "v3i0Wa-i-fp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert YearMonth to datetime if exists\n",
        "if \"YearMonth\" in df.columns:\n",
        "    df[\"YearMonth\"] = pd.to_datetime(df[\"YearMonth\"], errors=\"coerce\")\n",
        "else:\n",
        "    df[\"YearMonth\"] = pd.to_datetime(df[[\"Year\", \"Month\"]].assign(DAY=1))\n",
        "# Keep relevant columns\n",
        "assert \"Park\" in df.columns, \"Dataset must include 'Park' column\"\n",
        "assert \"Recreation Visits\" in df.columns, \"Dataset must include 'Recreation Visits' column\"\n",
        "\n",
        "df = df[[\"Park\", \"YearMonth\", \"Recreation Visits\"]].copy()\n",
        "df = df.dropna(subset=[\"YearMonth\"])\n",
        "df = df.sort_values([\"Park\", \"YearMonth\"])"
      ],
      "metadata": {
        "id": "tn6CIaL--vAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper metric\n",
        "def evaluate_forecast(y_true, y_pred):\n",
        "    # y_true and y_pred are pandas Series aligned by index\n",
        "    # if any naive mismatches, reindex\n",
        "    y_true, y_pred = y_true.align(y_pred, join=\"inner\")\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return mae, rmse"
      ],
      "metadata": {
        "id": "rSqvmzhd-5rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try imports for models\n",
        "# ETS\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# Prophet\n",
        "try:\n",
        "    try:\n",
        "        from prophet import Prophet\n",
        "    except Exception:\n",
        "        # older installation name\n",
        "        from fbprophet import Prophet\n",
        "    PROPHET_AVAILABLE = True\n",
        "except Exception:\n",
        "    PROPHET_AVAILABLE = False\n",
        "    RUN_PROPHET = False\n",
        "\n",
        "# pmdarima (auto_arima)\n",
        "try:\n",
        "    import pmdarima as pm\n",
        "    PM_AVAILABLE = True\n",
        "except Exception:\n",
        "    PM_AVAILABLE = False\n",
        "    RUN_SARIMA = False\n",
        "\n",
        "# LSTM\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    TF_AVAILABLE = True\n",
        "except Exception:\n",
        "    TF_AVAILABLE = False\n",
        "    RUN_LSTM = False"
      ],
      "metadata": {
        "id": "irJ2IGgY_AnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility: safe filename\n",
        "def safe_name(name):\n",
        "    return \"\".join(c if c.isalnum() or c in \"_- \" else \"_\" for c in name).replace(\" \", \"_\")[:120]"
      ],
      "metadata": {
        "id": "-zJY30YO_MZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function: prepare series per park\n",
        "def prepare_series(park_df, asfreq=\"MS\", fill_method=\"ffill\"):\n",
        "    s = park_df.set_index(\"YearMonth\")[\"Recreation Visits\"].sort_index()\n",
        "    s = s.asfreq(asfreq)  # missing months become NaN\n",
        "    # If there's leading NaNs, forward/backfill won't fill them - keep them as NaN\n",
        "    # Fill small gaps by forward fill, but leave series with too many missing as is\n",
        "    if fill_method == \"ffill\":\n",
        "        s = s.fillna(method=\"ffill\")\n",
        "    elif fill_method == \"interpolate\":\n",
        "        s = s.interpolate(limit_direction=\"both\")\n",
        "    return s"
      ],
      "metadata": {
        "id": "JaT7QZHo_QOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ETS model fit & forecast\n",
        "def fit_forecast_ets(train, horizon, seasonal_periods=12, seasonal=\"add\", trend=\"add\"):\n",
        "    # train: pd.Series with datetime index\n",
        "    model = ExponentialSmoothing(train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods)\n",
        "    fitted = model.fit(optimized=True, use_boxcox=False, remove_bias=False)\n",
        "    fc = fitted.forecast(horizon)\n",
        "    return fitted, fc\n",
        "\n",
        "# Prophet fit & forecast\n",
        "def fit_forecast_prophet(train, horizon):\n",
        "    # train: pd.Series with datetime index, monthly\n",
        "    # Prophet expects dataframe with ds, y\n",
        "    dfp = train.reset_index().rename(columns={\"YearMonth\": \"ds\", \"Recreation Visits\": \"y\"})\n",
        "    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
        "    m.fit(dfp)\n",
        "    future = m.make_future_dataframe(periods=horizon, freq=\"MS\")\n",
        "    fc = m.predict(future)\n",
        "    # return last horizon forecasts aligned to index\n",
        "    fc_series = pd.Series(fc[\"yhat\"].values[-horizon:], index=pd.date_range(start=train.index[-1] + pd.offsets.MonthBegin(1), periods=horizon, freq=\"MS\"))\n",
        "    return m, fc_series\n",
        "\n",
        "# SARIMA: use pmdarima.auto_arima if available\n",
        "def fit_forecast_sarima(train, horizon, seasonal_periods=12):\n",
        "    # train: series\n",
        "    model = pm.auto_arima(train, seasonal=True, m=seasonal_periods, error_action=\"ignore\", suppress_warnings=True, stepwise=True)\n",
        "    # get forecast\n",
        "    fc, conf_int = model.predict(n_periods=horizon, return_conf_int=True, alpha=0.05)\n",
        "    idx = pd.date_range(start=train.index[-1] + pd.offsets.MonthBegin(1), periods=horizon, freq=\"MS\")\n",
        "    fc_series = pd.Series(fc, index=idx)\n",
        "    return model, fc_series\n",
        "\n",
        "# LSTM: simple window-to-one model (one-step ahead repeated to produce multi-step by recursive forecasting)\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation=\"tanh\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "238uDOXX_VXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_supervised(series, lookback=12):\n",
        "    \"\"\"Return X, y arrays for one-step forecasting using lookback.\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(series) - lookback):\n",
        "        X.append(series[i:i + lookback])\n",
        "        y.append(series[i + lookback])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def fit_forecast_lstm(train, horizon, lookback=12, epochs=50, batch_size=16):\n",
        "    \"\"\"\n",
        "    We'll use recursive multi-step: predict 1-step ahead, append, and repeat.\n",
        "    train: pd.Series\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    train_vals = train.values.reshape(-1, 1)\n",
        "    scaled = scaler.fit_transform(train_vals).flatten()\n",
        "    if len(scaled) <= lookback:\n",
        "        raise ValueError(\"not enough data for LSTM with lookback\")\n",
        "    X, y = prepare_supervised(scaled, lookback)\n",
        "    # reshape for LSTM [samples, timesteps, features]\n",
        "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "    # simple train/val split\n",
        "    split = int(0.9 * len(X))\n",
        "    X_train, y_train = X[:split], y[:split]\n",
        "    X_val, y_val = X[split:], y[split:]\n",
        "    model = create_lstm_model((lookback, 1))\n",
        "    # callbacks\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=0)\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=0)\n",
        "    # recursive forecast\n",
        "    last_window = scaled[-lookback:].tolist()\n",
        "    preds = []\n",
        "    for _ in range(horizon):\n",
        "        x_input = np.array(last_window[-lookback:]).reshape((1, lookback, 1))\n",
        "        pred_scaled = model.predict(x_input, verbose=0)[0, 0]\n",
        "        preds.append(pred_scaled)\n",
        "        last_window.append(pred_scaled)\n",
        "    preds = np.array(preds).reshape(-1, 1)\n",
        "    preds_inv = scaler.inverse_transform(preds).flatten()\n",
        "    idx = pd.date_range(start=train.index[-1] + pd.offsets.MonthBegin(1), periods=horizon, freq=\"MS\")\n",
        "    fc_series = pd.Series(preds_inv, index=idx)\n",
        "    return model, fc_series"
      ],
      "metadata": {
        "id": "YV_F8cvZ_f3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C8Hpfwc9soA"
      },
      "outputs": [],
      "source": [
        "# MAIN LOOP: per park\n",
        "parks = df[\"Park\"].unique()\n",
        "results = []\n",
        "failed_parks = []\n",
        "\n",
        "for park in tqdm(parks, desc=\"Parks\"):\n",
        "    park_df = df[df[\"Park\"] == park][[\"YearMonth\", \"Recreation Visits\"]].copy()\n",
        "    series = prepare_series(park_df, asfreq=\"MS\", fill_method=\"ffill\")\n",
        "    # drop if too short overall\n",
        "    if series.dropna().shape[0] < MIN_HISTORY_MONTHS:\n",
        "        # still try very simple ETS if at least 12 months? else skip\n",
        "        if series.dropna().shape[0] < 12:\n",
        "            failed_parks.append((park, \"insufficient_history\"))\n",
        "            continue\n",
        "\n",
        "    # ensure no negative values\n",
        "    series = series.clip(lower=0)\n",
        "\n",
        "    # Optionally log transform\n",
        "    if USE_LOG_TRANSFORM:\n",
        "        series_trans = np.log1p(series)\n",
        "    else:\n",
        "        series_trans = series.copy()\n",
        "\n",
        "    # drop NaNs at end/start to allow train/test split\n",
        "    series_trans = series_trans.dropna()\n",
        "    if len(series_trans) < (HORIZON + 1):\n",
        "        failed_parks.append((park, \"insufficient_after_dropna\"))\n",
        "        continue\n",
        "\n",
        "    # split\n",
        "    train = series_trans.iloc[:-HORIZON]\n",
        "    test = series_trans.iloc[-HORIZON:]\n",
        "\n",
        "    park_results = {\"Park\": park, \"n_obs\": len(series_trans)}\n",
        "    model_forecasts = {}\n",
        "    model_metrics = {}\n",
        "\n",
        "    # ETS\n",
        "    if RUN_ETS:\n",
        "        try:\n",
        "            # choose seasonal type heuristically: multiplicative if coefficient of variation increases with level - optional\n",
        "            ets_model, ets_fc = fit_forecast_ets(train, HORIZON, seasonal_periods=12, seasonal=\"add\", trend=\"add\")\n",
        "            # invert transform if used\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                ets_fc_inv = np.expm1(ets_fc)\n",
        "                test_inv = np.expm1(test)\n",
        "            else:\n",
        "                ets_fc_inv = ets_fc\n",
        "                test_inv = test\n",
        "            mae, rmse = evaluate_forecast(test_inv, ets_fc_inv)\n",
        "            model_forecasts[\"ETS\"] = ets_fc_inv\n",
        "            model_metrics[\"ETS\"] = {\"MAE\": mae, \"RMSE\": rmse}\n",
        "        except Exception as e:\n",
        "            model_metrics[\"ETS\"] = {\"error\": str(e)}\n",
        "\n",
        "    # Prophet\n",
        "    if RUN_PROPHET and PROPHET_AVAILABLE:\n",
        "        try:\n",
        "            # Prophet wants original scale, so handle transforms properly\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                train_prop = np.expm1(train)\n",
        "            else:\n",
        "                train_prop = train\n",
        "            prophet_model, prophet_fc = fit_forecast_prophet(train_prop, HORIZON)\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                prophet_fc_used = prophet_fc  # prophet used inv transform earlier\n",
        "                test_inv = np.expm1(test)\n",
        "            else:\n",
        "                prophet_fc_used = prophet_fc\n",
        "                test_inv = test\n",
        "            mae, rmse = evaluate_forecast(test_inv, prophet_fc_used)\n",
        "            model_forecasts[\"Prophet\"] = prophet_fc_used\n",
        "            model_metrics[\"Prophet\"] = {\"MAE\": mae, \"RMSE\": rmse}\n",
        "        except Exception as e:\n",
        "            model_metrics[\"Prophet\"] = {\"error\": str(e)}\n",
        "\n",
        "    # SARIMA\n",
        "    if RUN_SARIMA and PM_AVAILABLE:\n",
        "        try:\n",
        "            # pmdarima expects numpy array without index\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                sar_train = np.expm1(train)\n",
        "            else:\n",
        "                sar_train = train\n",
        "            sar_model, sar_fc = fit_forecast_sarima(sar_train, HORIZON, seasonal_periods=12)\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                test_inv = np.expm1(test)\n",
        "            else:\n",
        "                test_inv = test\n",
        "            mae, rmse = evaluate_forecast(test_inv, sar_fc)\n",
        "            model_forecasts[\"SARIMA\"] = sar_fc\n",
        "            model_metrics[\"SARIMA\"] = {\"MAE\": mae, \"RMSE\": rmse}\n",
        "        except Exception as e:\n",
        "            model_metrics[\"SARIMA\"] = {\"error\": str(e)}\n",
        "\n",
        "    # LSTM\n",
        "    if RUN_LSTM and TF_AVAILABLE and len(series_trans.dropna()) >= LSTM_MIN_MONTHS:\n",
        "        try:\n",
        "            # LSTM works on original scale\n",
        "            lstm_train = np.expm1(train) if USE_LOG_TRANSFORM else train\n",
        "            lstm_model, lstm_fc = fit_forecast_lstm(lstm_train, HORIZON, lookback=LSTM_LOOKBACK, epochs=LSTM_EPOCHS, batch_size=LSTM_BATCH)\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                test_inv = np.expm1(test)\n",
        "            else:\n",
        "                test_inv = test\n",
        "            mae, rmse = evaluate_forecast(test_inv, lstm_fc)\n",
        "            model_forecasts[\"LSTM\"] = lstm_fc\n",
        "            model_metrics[\"LSTM\"] = {\"MAE\": mae, \"RMSE\": rmse}\n",
        "        except Exception as e:\n",
        "            model_metrics[\"LSTM\"] = {\"error\": str(e)}\n",
        "    else:\n",
        "        if RUN_LSTM and not TF_AVAILABLE:\n",
        "            model_metrics[\"LSTM\"] = {\"error\": \"tensorflow_not_available\"}\n",
        "        elif RUN_LSTM and len(series_trans.dropna()) < LSTM_MIN_MONTHS:\n",
        "            model_metrics[\"LSTM\"] = {\"error\": \"insufficient_history_for_lstm\"}\n",
        "\n",
        "    # Select best model by RMSE (lowest). models that raised errors are ignored\n",
        "    valid_models = {m: v for m, v in model_metrics.items() if \"RMSE\" in v}\n",
        "    if len(valid_models) == 0:\n",
        "        # No model successfully fit for this park\n",
        "        failed_parks.append((park, \"no_models_succeeded\", model_metrics))\n",
        "        # Add the park_results with model_metrics for debugging, but don't try to sort by RMSE later for this entry\n",
        "        results.append({**park_results, \"model_metrics\": model_metrics})\n",
        "        continue\n",
        "\n",
        "    # choose best\n",
        "    best_model_name = min(valid_models.keys(), key=lambda m: valid_models[m][\"RMSE\"])\n",
        "    best_metrics = valid_models[best_model_name]\n",
        "    park_results.update({\n",
        "        \"best_model\": best_model_name,\n",
        "        \"best_MAE\": best_metrics[\"MAE\"],\n",
        "        \"best_RMSE\": best_metrics[\"RMSE\"],\n",
        "    })\n",
        "\n",
        "    # Refit best model to full history (train + test) and produce final forecast for horizon\n",
        "    try:\n",
        "        full_series = series_trans.copy()\n",
        "        if best_model_name == \"ETS\":\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                full_for_fit = np.log1p(full_series)\n",
        "            else:\n",
        "                full_for_fit = full_series\n",
        "            fitted_full, fc_full = fit_forecast_ets(full_for_fit, HORIZON, seasonal_periods=12, seasonal=\"add\", trend=\"add\")\n",
        "            if USE_LOG_TRANSFORM:\n",
        "                fc_full = np.expm1(fc_full)\n",
        "        elif best_model_name == \"Prophet\":\n",
        "            # Prophet expects original scale\n",
        "            full_for_fit = np.expm1(full_series) if USE_LOG_TRANSFORM else full_series\n",
        "            m_full = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
        "            dfp = full_for_fit.reset_index().rename(columns={\"YearMonth\": \"ds\", \"Recreation Visits\": \"y\"})\n",
        "            m_full.fit(dfp)\n",
        "            future = m_full.make_future_dataframe(periods=HORIZON, freq=\"MS\")\n",
        "            fc = m_full.predict(future)\n",
        "            fc_full = pd.Series(fc[\"yhat\"].values[-HORIZON:], index=pd.date_range(start=full_series.index[-1] + pd.offsets.MonthBegin(1), periods=HORIZON, freq=\"MS\"))\n",
        "        elif best_model_name == \"SARIMA\":\n",
        "            full_for_fit = np.expm1(full_series) if USE_LOG_TRANSFORM else full_series\n",
        "            sar_full, fc_full = fit_forecast_sarima(full_for_fit, HORIZON, seasonal_periods=12)\n",
        "        elif best_model_name == \"LSTM\":\n",
        "            full_for_fit = np.expm1(full_series) if USE_LOG_TRANSFORM else full_series\n",
        "            lstm_full_model, fc_full = fit_forecast_lstm(full_for_fit, HORIZON, lookback=LSTM_LOOKBACK, epochs=LSTM_EPOCHS, batch_size=LSTM_BATCH)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown best model\")\n",
        "    except Exception as e:\n",
        "        park_results.update({\"refit_error\": str(e)})\n",
        "        # still save the test metrics as final\n",
        "        results.append({**park_results, **{\"model_metrics\": model_metrics}})\n",
        "        continue\n",
        "\n",
        "    # Save final forecast to CSV\n",
        "    fc_df = pd.DataFrame({\n",
        "        \"YearMonth\": fc_full.index,\n",
        "        \"Forecast\": fc_full.values\n",
        "    })\n",
        "    fc_path = FORECASTS_DIR / f\"{safe_name(park)}.csv\"\n",
        "    fc_df.to_csv(fc_path, index=False)\n",
        "\n",
        "    # Optionally save a small actual vs predicted plot for the test horizon\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        # plot last 36 months actual\n",
        "        window = 36\n",
        "        recent_index = series.index[-window:]\n",
        "        plt.plot(series.loc[recent_index], label=\"Actual\")\n",
        "        # plot the test predicted from best model (from earlier stored model_forecasts)\n",
        "        if best_model_name in model_forecasts:\n",
        "            plt.plot(model_forecasts[best_model_name], label=f\"Predicted ({best_model_name}) - test\")\n",
        "        # plot the final forecast (fc_full)\n",
        "        plt.plot(fc_full, \"--\", label=f\"Forecast ({best_model_name})\")\n",
        "        plt.title(f\"{park} â€” Best: {best_model_name}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(PLOTS_DIR / f\"{safe_name(park)}.png\")\n",
        "        plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # store results\n",
        "    park_results[\"model_metrics\"] = model_metrics\n",
        "    park_results[\"forecast_csv\"] = str(fc_path)\n",
        "    results.append(park_results)\n",
        "\n",
        "# Save summary results to CSV (flatten)\n",
        "summary_rows = []\n",
        "for r in results:\n",
        "    row = {\n",
        "        \"Park\": r[\"Park\"],\n",
        "        \"n_obs\": r.get(\"n_obs\", None),\n",
        "        \"best_model\": r.get(\"best_model\", None),\n",
        "        \"best_MAE\": r.get(\"best_MAE\", None),\n",
        "        \"best_RMSE\": r.get(\"best_RMSE\", None), # Use .get() with default None\n",
        "        \"forecast_csv\": r.get(\"forecast_csv\", None),\n",
        "    }\n",
        "    summary_rows.append(row)\n",
        "\n",
        "# Filter out rows where best_RMSE is None before sorting\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df_sortable = summary_df.dropna(subset=[\"best_RMSE\"])\n",
        "\n",
        "if len(summary_df_sortable) > 0:\n",
        "    summary_df_sortable = summary_df_sortable.sort_values(\"best_RMSE\")\n",
        "\n",
        "# Concatenate the sortable and non-sortable rows for the final summary\n",
        "summary_df = pd.concat([summary_df_sortable, summary_df[summary_df[\"best_RMSE\"].isna()]])\n",
        "\n",
        "summary_df.to_csv(RESULTS_CSV, index=False)\n",
        "\n",
        "# Report\n",
        "print(\"Done.\")\n",
        "print(f\"Output directory: {OUTPUT_DIR.resolve()}\")\n",
        "print(f\"Model selection summary: {RESULTS_CSV}\")\n",
        "print(f\"Forecasts directory: {FORECASTS_DIR}\")\n",
        "print(f\"Plots directory: {PLOTS_DIR}\")\n",
        "print(f\"Failed parks count: {len(failed_parks)} (details in memory)\")\n",
        "\n",
        "# Optionally, display top 10 parks and which model was best\n",
        "if len(summary_df_sortable) > 0:\n",
        "    print(\"\\nTop 10 parks by lowest RMSE (best-fit):\")\n",
        "    print(summary_df_sortable[[\"Park\", \"best_model\", \"best_MAE\", \"best_RMSE\"]].head(10).to_string(index=False))\n",
        "\n",
        "# Save failed parks info for debugging\n",
        "failed_df = pd.DataFrame(failed_parks, columns=[\"Park\", \"Reason\", \"Details\"]).head(200)\n",
        "failed_df.to_csv(OUTPUT_DIR / \"failed_parks_preview.csv\", index=False)"
      ]
    }
  ]
}